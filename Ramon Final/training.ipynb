{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time, copy\n",
    "from importlib import reload\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim, nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_path = \"./data/combined/\"\n",
    "max_vals_path = \"./data/max_vals.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.83846143e+03 7.32256397e+03 4.54205127e+03 4.23384619e+03\n",
      " 4.35384619e+03 7.90871777e+03 8.27025684e+03 6.21897412e+03\n",
      " 6.23282031e+03 6.60512842e+03 5.28617189e+03 7.74127599e+03\n",
      " 5.04152043e+03 4.94588142e+03 5.15734374e+03 3.60216976e+06\n",
      " 9.17141303e+06 1.10068438e+06 1.05882940e+06 1.17023543e+06\n",
      " 5.27384619e+03 7.77179492e+03 5.02282056e+03 4.93000000e+03\n",
      " 5.14538477e+03 6.97277984e+03 7.43616581e+02 1.46733224e+02\n",
      " 3.27454090e+01 1.86658591e+01 1.92897892e+04 5.62001199e+03\n",
      " 2.05430640e+03 1.00586385e+03 2.35064302e+02 1.35319137e+03\n",
      " 3.07286039e+02 1.33131847e+02 8.05779346e+01 4.93367400e+01\n",
      " 1.14952310e+03 2.76171629e+02 1.22808444e+02 3.07193409e+01\n",
      " 2.29274350e+01 9.83024695e+02 2.40093994e+02 1.13710491e+02\n",
      " 3.39073937e+01 2.46449293e+01]\n",
      "[1.00000000e+00 4.21794873e+03 4.12820508e+03 3.99435889e+03\n",
      " 4.10205127e+03 4.16153857e+03 4.35435889e+03 4.36051270e+03\n",
      " 4.52717969e+03 4.23538477e+03 4.28923096e+03 4.27457931e+03\n",
      " 4.25288463e+03 4.19854167e+03 4.15399840e+03 4.22287259e+03\n",
      " 6.28107562e+02 2.01465442e+03 8.30888087e+03 6.24660701e+02\n",
      " 8.34595268e+02 4.27512817e+03 4.25948730e+03 4.16948730e+03\n",
      " 4.15230762e+03 4.21871777e+03 6.62511494e+00 6.88605075e+00\n",
      " 1.38178575e+00 1.81844388e+00 1.48487281e+00 6.09523563e+00\n",
      " 3.30341931e+00 3.26024581e+00 4.28909856e+00 4.59663937e+00\n",
      " 3.06332345e+01 1.66878063e+01 6.13886475e+00 4.46861175e+00\n",
      " 1.81736063e+00 3.66763619e+00 2.44367419e+00 2.12872325e+00\n",
      " 2.29658850e+00 1.64498800e+00 6.83125294e+00 6.26407144e+00\n",
      " 1.41458994e+00 1.85343619e+00 1.21891613e+00]\n"
     ]
    }
   ],
   "source": [
    "max_vals = np.load(max_vals_path)\n",
    "print(max_vals)\n",
    "\n",
    "example = np.load(combined_path + \"255.npy\")\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path, mode, max_path=None, transform=None):\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        \n",
    "        train = []\n",
    "        test = []\n",
    "        \n",
    "        self.max_features = np.load(max_path)\n",
    "        \n",
    "        for filename in os.listdir(data_path):\n",
    "            index = int(filename[:-4])\n",
    "            if index % 10 == 0:\n",
    "                test.append(np.load(data_path+filename))\n",
    "            else:\n",
    "                train.append(np.load(data_path+filename))\n",
    "                \n",
    "        self.data = train if mode == \"train\" else test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        #print(\"HELLO\")\n",
    "        #print(\"This is shape\", np.array(self.data).shape)\n",
    "        data = self.data[idx][1:].astype(np.float)\n",
    "        data = data / self.max_features\n",
    "        #why are we dividing by the max?\n",
    "        label = self.data[idx][0]\n",
    "        #stored as the label\n",
    "        \n",
    "        if self.transform != None:\n",
    "            data = self.transform(label)\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 1064, 'test': 118}\n"
     ]
    }
   ],
   "source": [
    "train_set = FeatureDataset(combined_path,\"train\",max_path=max_vals_path)\n",
    "train_loader = DataLoader(train_set, batch_size=16, num_workers=4, shuffle=True)\n",
    "\n",
    "test_set = FeatureDataset(combined_path,\"test\",max_path=max_vals_path)\n",
    "test_loader = DataLoader(test_set, batch_size=16, num_workers=4)\n",
    "\n",
    "dataloaders = {\"train\": train_loader, \"test\": test_loader}\n",
    "dataset_sizes = {\"train\": len(train_set), \"test\": len(test_set)}\n",
    "\n",
    "print(dataset_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    #print(i)\n",
    "    x = train_set[i]\n",
    "    #print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, dataloaders, dataset_sizes, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'test']:\n",
    "            model.train()\n",
    "    \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                #print(\"prev input.shape{}\".format(inputs.shape))\n",
    "                inputs = inputs.float().squeeze()\n",
    "                #print(\"after input.shape{}\".format(inputs.shape))\n",
    "                #print(\"prev labels.shape{}\".format(labels.shape))\n",
    "                labels = labels.float().squeeze()\n",
    "                #print(\"after labels.shape{}\".format(labels.shape))\n",
    "\n",
    "                # forward\n",
    "                # Only calculate gradient for training\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs).squeeze()  \n",
    "                    #calcuting the loss based on the BCE Criterion\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    preds = torch.round(outputs)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item()\n",
    "                running_corrects += torch.sum(preds == labels)\n",
    "                \n",
    "            if phase == 'train' and scheduler != None:\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'test' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=50, out_features=10, bias=False)\n",
      "  (1): Linear(in_features=10, out_features=1, bias=False)\n",
      "  (2): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "my_model = nn.Sequential(nn.Linear(50,10,bias=False),nn.Linear(10,1,bias=False), nn.Sigmoid())\n",
    "criterion = nn.BCELoss()\n",
    "print(my_model)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "# optimizer_ft = optim.SGD(my_model.parameters(), lr=0.5)\n",
    "optimizer_ft = optim.Adam(my_model.parameters(),lr=0.05, weight_decay=0.0001)\n",
    "lr_scheduler = None\n",
    "# lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=25, gamma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 0.0443 Acc: 0.5103\n",
      "test Loss: 0.0475 Acc: 0.5085\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 0.0392 Acc: 0.6419\n",
      "test Loss: 0.0388 Acc: 0.6695\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.0394 Acc: 0.6325\n",
      "test Loss: 0.0389 Acc: 0.6610\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.0385 Acc: 0.6410\n",
      "test Loss: 0.0392 Acc: 0.7119\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.0382 Acc: 0.6692\n",
      "test Loss: 0.0443 Acc: 0.4915\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.0361 Acc: 0.6936\n",
      "test Loss: 0.0384 Acc: 0.7119\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.0373 Acc: 0.6739\n",
      "test Loss: 0.0379 Acc: 0.6780\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.0344 Acc: 0.7002\n",
      "test Loss: 0.0444 Acc: 0.6017\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.0375 Acc: 0.6795\n",
      "test Loss: 0.0402 Acc: 0.6949\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.0366 Acc: 0.6880\n",
      "test Loss: 0.0379 Acc: 0.7288\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.0350 Acc: 0.6870\n",
      "test Loss: 0.0369 Acc: 0.7458\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.0352 Acc: 0.6880\n",
      "test Loss: 0.0438 Acc: 0.5932\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.0353 Acc: 0.7077\n",
      "test Loss: 0.0373 Acc: 0.7203\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.0349 Acc: 0.7227\n",
      "test Loss: 0.0388 Acc: 0.7373\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.0332 Acc: 0.7293\n",
      "test Loss: 0.0363 Acc: 0.7034\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.0326 Acc: 0.7284\n",
      "test Loss: 0.0406 Acc: 0.7119\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.0348 Acc: 0.7068\n",
      "test Loss: 0.0396 Acc: 0.6949\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.0331 Acc: 0.7209\n",
      "test Loss: 0.0358 Acc: 0.7458\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.0330 Acc: 0.7293\n",
      "test Loss: 0.0397 Acc: 0.6610\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.0337 Acc: 0.7237\n",
      "test Loss: 0.0428 Acc: 0.6271\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 0.0341 Acc: 0.7077\n",
      "test Loss: 0.0355 Acc: 0.7373\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.0335 Acc: 0.7086\n",
      "test Loss: 0.0356 Acc: 0.7203\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.0325 Acc: 0.7340\n",
      "test Loss: 0.0351 Acc: 0.7288\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.0310 Acc: 0.7556\n",
      "test Loss: 0.0364 Acc: 0.7458\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.0317 Acc: 0.7425\n",
      "test Loss: 0.0343 Acc: 0.7203\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.0348 Acc: 0.7021\n",
      "test Loss: 0.0352 Acc: 0.7542\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.0321 Acc: 0.7425\n",
      "test Loss: 0.0348 Acc: 0.7203\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.0312 Acc: 0.7491\n",
      "test Loss: 0.0373 Acc: 0.7119\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.0315 Acc: 0.7378\n",
      "test Loss: 0.0366 Acc: 0.7797\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.0309 Acc: 0.7500\n",
      "test Loss: 0.0399 Acc: 0.7034\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 0.0337 Acc: 0.7124\n",
      "test Loss: 0.0352 Acc: 0.7373\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.0308 Acc: 0.7575\n",
      "test Loss: 0.0348 Acc: 0.7542\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.0309 Acc: 0.7538\n",
      "test Loss: 0.0356 Acc: 0.7542\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.0311 Acc: 0.7425\n",
      "test Loss: 0.0365 Acc: 0.7542\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.0302 Acc: 0.7594\n",
      "test Loss: 0.0446 Acc: 0.7119\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.0321 Acc: 0.7415\n",
      "test Loss: 0.0354 Acc: 0.7542\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.0323 Acc: 0.7331\n",
      "test Loss: 0.0349 Acc: 0.7458\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.0306 Acc: 0.7444\n",
      "test Loss: 0.0436 Acc: 0.6695\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.0325 Acc: 0.7274\n",
      "test Loss: 0.0365 Acc: 0.7712\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.0308 Acc: 0.7491\n",
      "test Loss: 0.0387 Acc: 0.7119\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 0.0299 Acc: 0.7566\n",
      "test Loss: 0.0360 Acc: 0.7881\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.0312 Acc: 0.7387\n",
      "test Loss: 0.0347 Acc: 0.7712\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.0294 Acc: 0.7744\n",
      "test Loss: 0.0339 Acc: 0.7712\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.0327 Acc: 0.7237\n",
      "test Loss: 0.0375 Acc: 0.6949\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.0309 Acc: 0.7378\n",
      "test Loss: 0.0351 Acc: 0.7627\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.0294 Acc: 0.7697\n",
      "test Loss: 0.0336 Acc: 0.7458\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.0313 Acc: 0.7538\n",
      "test Loss: 0.0359 Acc: 0.7542\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.0319 Acc: 0.7218\n",
      "test Loss: 0.0347 Acc: 0.7458\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.0327 Acc: 0.7284\n",
      "test Loss: 0.0353 Acc: 0.7627\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 0.0300 Acc: 0.7669\n",
      "test Loss: 0.0407 Acc: 0.7034\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 0.0301 Acc: 0.7650\n",
      "test Loss: 0.0362 Acc: 0.7542\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 0.0303 Acc: 0.7547\n",
      "test Loss: 0.0392 Acc: 0.7119\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 0.0354 Acc: 0.7058\n",
      "test Loss: 0.0371 Acc: 0.7203\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 0.0307 Acc: 0.7547\n",
      "test Loss: 0.0345 Acc: 0.7712\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 0.0294 Acc: 0.7679\n",
      "test Loss: 0.0346 Acc: 0.7797\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 0.0328 Acc: 0.7331\n",
      "test Loss: 0.0355 Acc: 0.7797\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 0.0325 Acc: 0.7378\n",
      "test Loss: 0.0415 Acc: 0.6864\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 0.0321 Acc: 0.7444\n",
      "test Loss: 0.0382 Acc: 0.7288\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 0.0325 Acc: 0.7406\n",
      "test Loss: 0.0421 Acc: 0.7119\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 0.0306 Acc: 0.7509\n",
      "test Loss: 0.0338 Acc: 0.7542\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 0.0297 Acc: 0.7594\n",
      "test Loss: 0.0342 Acc: 0.7627\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 0.0306 Acc: 0.7481\n",
      "test Loss: 0.0455 Acc: 0.6017\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 0.0323 Acc: 0.7453\n",
      "test Loss: 0.0340 Acc: 0.7542\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 0.0337 Acc: 0.7246\n",
      "test Loss: 0.0346 Acc: 0.7458\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 0.0294 Acc: 0.7632\n",
      "test Loss: 0.0343 Acc: 0.7797\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 0.0296 Acc: 0.7735\n",
      "test Loss: 0.0350 Acc: 0.7627\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 0.0304 Acc: 0.7679\n",
      "test Loss: 0.0355 Acc: 0.7542\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 0.0304 Acc: 0.7462\n",
      "test Loss: 0.0353 Acc: 0.7542\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 0.0305 Acc: 0.7622\n",
      "test Loss: 0.0370 Acc: 0.7542\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 0.0297 Acc: 0.7632\n",
      "test Loss: 0.0390 Acc: 0.7119\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 0.0286 Acc: 0.7801\n",
      "test Loss: 0.0377 Acc: 0.7542\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 0.0296 Acc: 0.7688\n",
      "test Loss: 0.0365 Acc: 0.7542\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 0.0294 Acc: 0.7613\n",
      "test Loss: 0.0370 Acc: 0.7542\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 0.0320 Acc: 0.7397\n",
      "test Loss: 0.0361 Acc: 0.7203\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 0.0298 Acc: 0.7697\n",
      "test Loss: 0.0351 Acc: 0.7373\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 0.0300 Acc: 0.7538\n",
      "test Loss: 0.0341 Acc: 0.7542\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 0.0308 Acc: 0.7538\n",
      "test Loss: 0.0345 Acc: 0.7712\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 0.0297 Acc: 0.7660\n",
      "test Loss: 0.0361 Acc: 0.7203\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 0.0290 Acc: 0.7829\n",
      "test Loss: 0.0334 Acc: 0.7627\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 0.0297 Acc: 0.7650\n",
      "test Loss: 0.0344 Acc: 0.7542\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 0.0295 Acc: 0.7528\n",
      "test Loss: 0.0341 Acc: 0.7797\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 0.0306 Acc: 0.7528\n",
      "test Loss: 0.0524 Acc: 0.5763\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 0.0300 Acc: 0.7660\n",
      "test Loss: 0.0346 Acc: 0.7712\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 0.0313 Acc: 0.7669\n",
      "test Loss: 0.0391 Acc: 0.7119\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 0.0309 Acc: 0.7481\n",
      "test Loss: 0.0360 Acc: 0.7373\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 0.0289 Acc: 0.7763\n",
      "test Loss: 0.0391 Acc: 0.7373\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 0.0301 Acc: 0.7519\n",
      "test Loss: 0.0368 Acc: 0.7966\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 0.0304 Acc: 0.7462\n",
      "test Loss: 0.0367 Acc: 0.7458\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 0.0300 Acc: 0.7547\n",
      "test Loss: 0.0354 Acc: 0.7712\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 0.0319 Acc: 0.7378\n",
      "test Loss: 0.0401 Acc: 0.7034\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 0.0300 Acc: 0.7726\n",
      "test Loss: 0.0452 Acc: 0.6102\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 0.0318 Acc: 0.7481\n",
      "test Loss: 0.0398 Acc: 0.7034\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 0.0294 Acc: 0.7707\n",
      "test Loss: 0.0333 Acc: 0.7627\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 0.0287 Acc: 0.7782\n",
      "test Loss: 0.0360 Acc: 0.7542\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 0.0318 Acc: 0.7397\n",
      "test Loss: 0.0352 Acc: 0.7712\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 0.0293 Acc: 0.7763\n",
      "test Loss: 0.0348 Acc: 0.7627\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 0.0288 Acc: 0.7744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.0364 Acc: 0.7627\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 0.0287 Acc: 0.7632\n",
      "test Loss: 0.0342 Acc: 0.7966\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 0.0295 Acc: 0.7575\n",
      "test Loss: 0.0398 Acc: 0.7119\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 0.0289 Acc: 0.7716\n",
      "test Loss: 0.0338 Acc: 0.7966\n",
      "\n",
      "Training complete in 0m 18s\n",
      "Best val Acc: 0.796610\n"
     ]
    }
   ],
   "source": [
    "final_model = train_model(my_model, \n",
    "                                                       criterion, \n",
    "                                                       optimizer_ft, \n",
    "                                                       lr_scheduler, \n",
    "                                                       dataloaders, \n",
    "                                                       dataset_sizes,\n",
    "                                                       num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(final_model.state_dict(), \"models/final.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
